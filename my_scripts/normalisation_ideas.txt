1.) For comparing with existing tagged database-forms (which aren't normalised before comparing), you could try just every single permutation of ъ/ь/e/o for every jer in the source-word (wouldnt help with fully-lost jers and would be quite wasteful when converting ъ to е (and ь => o but that would only be relevant for East Slavic and hardly even then)).


2.) You could get rid of ѕ and the things you convert to it entirely and replace with з, because it's only in Mar. and Psal. that it really occurs very often where it should. You need a less conservative lemma-orthography to check against because converting forms forwards is easier than going backwards
щ > шт, also in lemmas (get fockin rid)
jer+vowel should be swapped out with (all possible forms of) <i>+vowel

3.) maybe to deal with ъи = /y/ in Psal. and Kiev. we should just revert to viewing this as a diphthong and convert all possible representations of this sound to ъі, and all other <i> variants also to this Cyrillic і.

3.) the problem with normalising the training-data is it's still going to lead to loads of equivalent forms being spelt differently, and I don't think TnT pays any attention to how similar the byte-sequence of each word is to another, it views them atomatically, so really the normalisation needs to be as aggressive and modernising as possible

4.) Once I have enough LCS reconstructions I could even start training a model that just goes directly from a word's spelling to its LCS reconstruction


